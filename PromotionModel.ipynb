{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data set, do a quick exploratory data analysis to get a feel for the distributions and biases of the data.  Report any visualizations and findings used and suggest any other impactful business use cases for that data:\n",
    "\n",
    "There is a lot of imbalance in the distribution of peoples year that they are in with\n",
    "((2719 + 2273)/(2719 + 2273+5+3) = .9984) 99.84% of the people chosen being year 2 and year 3. In addition, the vast majority of the data came from these three schools Butler University (1614/5000), Indiana State University (1309/5000), and Ball State University (1085/5000). This is also shown to be visualized in the majors, with there not being more than 76 people in the bottom 9 majors shown (like Psychology and civil engineering). This makes a bias by there being an over representation of people in year 2 and 3 or people from those specific universities/majors and causes the dataset to primarily reflect the characteristics of individuals from these academic years and institutions and not represent a year one as well. Given this data, the more popular universities, like butler, should be focused as they have more data surrounding it. Also if other universities have the possibility of being added, they should consider factors like the proportions of people that are year 2 and year 3 and if there major is a popular major in the dataset like chem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider implications of data collection, storage, and data biases you would consider relevant here considering Data Ethics, Business Outcomes, and Technical Implications\n",
    "\n",
    "Discuss Ethical implications of these factors:\n",
    "\n",
    "    - Data Collection:\n",
    "        + Data could be collected in a way where a certain group of people are not collected and lead to an issue\n",
    "    - storage\n",
    "        + If the data is not stored properly, user information could be leaked \n",
    "    - biases\n",
    "        + A bias could lead to particular groups of people to not be represented properly and could create problems\n",
    "        \n",
    "Discuss Business outcome implications of these factors:\n",
    "\n",
    "    - Data Collection:\n",
    "        + A person can lie about their information and lead to the model predict on information that is not true. \n",
    "    - storage\n",
    "        + If the data isnt stored securely, someone can use this and put in information that leads to another option, or they could change data that is already there, which could remove possible patterns in the data.\n",
    "    - biases\n",
    "        + A data bias could lead to a less reliable model as it could not represent the desired population\n",
    "\n",
    "Discuss Technical implications of these factors\n",
    "\n",
    "    - Data Collection:\n",
    "        + make sure that data is collected in a way that gets the desired group of people to get a complete overview of the population\n",
    "    - storage\n",
    "        + Steps should be taken so only people who should see the data, gets to see it\n",
    "    - biases\n",
    "        + Systems could be in place to see whether biases are showing up in one way or the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = \"XTern 2024 Artificial Intelegence Data Set - Xtern_TrainData.csv\"\n",
    "data = pd.read_csv(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution of peoples year:\n",
      "Year 3: 2719\n",
      "Year 2: 2273\n",
      "Year 1: 5\n",
      "Year 4: 3\n",
      "\n",
      "distribution of peoples Major:\n",
      "Chemistry: 640\n",
      "Biology: 635\n",
      "Astronomy: 619\n",
      "Physics: 610\n",
      "Mathematics: 582\n",
      "Economics: 511\n",
      "Business Administration: 334\n",
      "Political Science: 309\n",
      "Marketing: 239\n",
      "Anthropology: 146\n",
      "Finance: 135\n",
      "Psychology: 76\n",
      "Accounting: 62\n",
      "Sociology: 31\n",
      "International Business: 29\n",
      "Music: 21\n",
      "Mechanical Engineering: 11\n",
      "Philosophy: 4\n",
      "Fine Arts: 3\n",
      "Civil Engineering: 3\n",
      "\n",
      "distribution of peoples Uni:\n",
      "Butler University: 1614\n",
      "Indiana State University: 1309\n",
      "Ball State University: 1085\n",
      "Indiana University-Purdue University Indianapolis (IUPUI): 682\n",
      "University of Notre Dame: 144\n",
      "University of Evansville: 143\n",
      "Indiana University Bloomington: 12\n",
      "Valparaiso University: 9\n",
      "Purdue University: 1\n",
      "DePauw University: 1\n",
      "\n",
      "distribution of peoples Time:\n",
      "13: 1316\n",
      "12: 1314\n",
      "14: 883\n",
      "11: 857\n",
      "15: 282\n",
      "10: 247\n",
      "16: 49\n",
      "9: 40\n",
      "8: 8\n",
      "17: 4\n"
     ]
    }
   ],
   "source": [
    "yearDist = data['Year'].value_counts().to_dict()\n",
    "majorDist = data['Major'].value_counts().to_dict()\n",
    "universityDist = data['University'].value_counts().to_dict()\n",
    "timeDist = data['Time'].value_counts().to_dict()\n",
    "\n",
    "\n",
    "print(\"distribution of peoples year:\")\n",
    "for element, count in yearDist.items():\n",
    "    print(f\"{element}: {count}\")\n",
    "\n",
    "print(\"\\ndistribution of peoples Major:\")\n",
    "for element, count in majorDist.items():\n",
    "    print(f\"{element}: {count}\")\n",
    "\n",
    "print(\"\\ndistribution of peoples Uni:\")\n",
    "for element, count in universityDist.items():\n",
    "    print(f\"{element}: {count}\")\n",
    "\n",
    "print(\"\\ndistribution of peoples Time:\")\n",
    "for element, count in timeDist.items():\n",
    "    print(f\"{element}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop('Order', axis=1)\n",
    "y = data['Order']\n",
    "\n",
    "enc = LabelEncoder()\n",
    "xEncoded = x.apply(lambda col: enc.fit_transform(col) if col.dtype == 'O' else col)\n",
    "\n",
    "# splits data into train and test split (90 10 split)\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(xEncoded, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best options are:  {'C': 15, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "The test accuracy is: 0.67\n"
     ]
    }
   ],
   "source": [
    "# sets the options to go through\n",
    "param_grid = {\n",
    "    'C': [5, 10, 15, 20],\n",
    "    'kernel': ['rbf'],\n",
    "    'gamma': ['auto']\n",
    "}\n",
    "# goes through each of the options looking for the combination that gives the best accuracy\n",
    "svmClassifier = SVC(random_state=42)\n",
    "gridSearch = GridSearchCV(svmClassifier, param_grid, cv=5, scoring='accuracy')\n",
    "gridSearch.fit(xTrain, yTrain)\n",
    "\n",
    "\n",
    "bestOptions = gridSearch.best_params_ # displays the best parameters\n",
    "print(f'The best options are:  {bestOptions}')\n",
    "\n",
    "# After the best parameters are found, the final model is created with the best parameters found\n",
    "svmBest = SVC(random_state=42, **bestOptions, probability=True)\n",
    "svmBest.fit(xTrain, yTrain)# model is trained on the training data\n",
    "\n",
    "# After the model is created and trained, it is tested on the test data for the accuracy\n",
    "testPredictions = svmBest.predict(xTest) \n",
    "\n",
    "testAccuracy = accuracy_score(yTest, testPredictions) # gets and displays accuracy\n",
    "print(f'The test accuracy is: {testAccuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svmBest, \"svm_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[29  3  3  0  1  7  0  4  0  2]\n",
      " [ 4 37  1  0  4  6  2  4  0  1]\n",
      " [ 0  8 34  1  0  1  0  0  4  2]\n",
      " [ 0  0  0 32  4  0  1  5  9  0]\n",
      " [ 0 10  1  5 25  2  0  2  0  5]\n",
      " [ 0  0  0  0  0 36  3  0  0  1]\n",
      " [ 0  2  1  0  1  1 46  0  0  0]\n",
      " [ 2  8  0  8  1  4  1 32  0  0]\n",
      " [ 0  2  3  3  0  0  2  5 31  2]\n",
      " [ 2  4  2  0  2  3  0  0  0 33]]\n"
     ]
    }
   ],
   "source": [
    "# displays the confusion matrix of the results\n",
    "confusion_mat = confusion_matrix(yTest, testPredictions)\n",
    "print(f'Confusion Matrix:\\n{confusion_mat}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "distribution of peoples Orders:\n",
      "Sugar Cream Pie: 512\n",
      "Indiana Pork Chili: 510\n",
      "Cornbread Hush Puppies: 510\n",
      "Sweet Potato Fries: 508\n",
      "Ultimate Grilled Cheese Sandwich (with bacon and tomato): 503\n",
      "Indiana Buffalo Chicken Tacos (3 tacos): 496\n",
      "Indiana Corn on the Cob (brushed with garlic butter): 495\n",
      "Breaded Pork Tenderloin Sandwich: 494\n",
      "Fried Catfish Basket: 490\n",
      "Hoosier BBQ Pulled Pork Sandwich: 482\n"
     ]
    }
   ],
   "source": [
    "timeDist = data['Order'].value_counts().to_dict()\n",
    "\n",
    "print(\"\\ndistribution of peoples Orders:\")\n",
    "for element, count in timeDist.items():\n",
    "    print(f\"{element}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what considerations would you make to determine if this is a suitable course of action?\n",
    "\n",
    "Given that the test accuracy of 67 percent, that means that as time passes 1 of every 3 people will get a 10 percent discount on their order. So in order for it to be worth it, the promotion needs to attract enough new users to counteract the loss of revenue from the discounts given out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
